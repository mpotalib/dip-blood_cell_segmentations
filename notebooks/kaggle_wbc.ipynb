{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WBC Segmentation (Unstained) - Kaggle GPU Notebook\n",
    "\n",
    "Run this end-to-end in a Kaggle Notebook with GPU. Adjust the dataset path as noted below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repo if not already present (Kaggle starts in /kaggle/working)\n",
    "import os, subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "REPO_URL = \"https://github.com/mpotalib/dip-blood_cell_segmentations.git\"\n",
    "REPO_DIR = Path(\"/kaggle/working/dip-blood_cell_segmentations\")\n",
    "\n",
    "if not REPO_DIR.exists():\n",
    "    subprocess.run([\"git\", \"clone\", REPO_URL, str(REPO_DIR)], check=True)\n",
    "os.chdir(REPO_DIR)\n",
    "print(\"CWD:\", Path.cwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857de723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point to your Kaggle dataset containing data/train|val|test with images + masks (or annotations)\n",
    "# Example: upload a dataset and set DATASET_BASE to /kaggle/input/your-dataset-name\n",
    "from pathlib import Path\n",
    "import os, shutil\n",
    "\n",
    "DATASET_BASE = Path(\"/kaggle/input/dip-wbc-dataset\")  # TODO: set to your dataset name\n",
    "TARGET = Path(\"data\")\n",
    "\n",
    "if not DATASET_BASE.exists():\n",
    "    raise FileNotFoundError(f\"Dataset path not found: {DATASET_BASE}. Update DATASET_BASE above.\")\n",
    "\n",
    "# If data is already structured as data/train/images etc, just symlink\n",
    "if TARGET.exists():\n",
    "    if TARGET.is_symlink():\n",
    "        TARGET.unlink()\n",
    "    else:\n",
    "        shutil.rmtree(TARGET)\n",
    "os.symlink(DATASET_BASE, TARGET)\n",
    "print(\"Linked\", DATASET_BASE, \"->\", TARGET)\n",
    "\n",
    "# If masks are not pre-generated but annotations exist, create masks\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    img_dir = TARGET / split / \"images\"\n",
    "    ann_dir = TARGET / split / \"annotations\"\n",
    "    mask_dir = TARGET / split / \"masks\"\n",
    "    if img_dir.exists() and ann_dir.exists() and not mask_dir.exists():\n",
    "        mask_dir.mkdir(parents=True, exist_ok=True)\n",
    "        !python prepare_masks.py --images-dir {img_dir} --annotations-dir {ann_dir} --output-dir {mask_dir}\n",
    "    else:\n",
    "        print(f\"Split {split}: images={img_dir.exists()}, annotations={ann_dir.exists()}, masks={mask_dir.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fce12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train (choose config: baseline or deeplab)\n",
    "!python train.py --config experiments/deeplab.yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be815e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on val/test and export qualitative masks for the report/deck\n",
    "!python evaluate.py --config experiments/deeplab.yaml --checkpoint outputs/deeplab/checkpoints/best.pt --split val --save-dir outputs/deeplab/preds_val --limit 20\n",
    "!python evaluate.py --config experiments/deeplab.yaml --checkpoint outputs/deeplab/checkpoints/best.pt --split test --save-dir outputs/deeplab/preds_test --limit 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6ce68d",
   "metadata": {},
   "source": [
    "## Train with log and plot curves\n",
    "Use tee to capture stdout to train.log, then parse and plot train/val curves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f73afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DeepLab and save log for plotting\n",
    "!python train.py --config experiments/deeplab.yaml | tee train.log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e011ddca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse train.log and plot loss/Dice curves\n",
    "import re, matplotlib.pyplot as plt\n",
    "train_loss, val_loss, val_dice = [], [], []\n",
    "with open('train.log') as f:\n",
    "    for line in f:\n",
    "        m = re.search(r'Epoch (\\d+)/(\\d+).*train loss ([0-9.]+)', line)\n",
    "        if m:\n",
    "            train_loss.append(float(m.group(3)))\n",
    "        m = re.search(r'Validation - loss: ([0-9.]+) \\| dice: ([0-9.]+)', line)\n",
    "        if m:\n",
    "            val_loss.append(float(m.group(1)))\n",
    "            val_dice.append(float(m.group(2)))\n",
    "if train_loss and val_loss:\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(train_loss, label='train loss')\n",
    "    plt.plot(val_loss, label='val loss')\n",
    "    plt.xlabel('epoch'); plt.ylabel('loss'); plt.legend(); plt.tight_layout()\n",
    "if val_dice:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(val_dice, label='val Dice')\n",
    "    plt.xlabel('epoch'); plt.ylabel('Dice'); plt.legend(); plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6884cb1e",
   "metadata": {},
   "source": [
    "## Visualizations and Metrics\n",
    "Per-class metrics, overlays, augmentation previews, and worst cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e184cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute per-class metrics on val and list worst cases by Dice\n",
    "import torch, yaml, numpy as np\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "from src.dataset import WBCDataset\n",
    "from src.transforms import get_val_transforms\n",
    "from src.models import build_model\n",
    "from src.utils import set_seed\n",
    "\n",
    "cfg = yaml.safe_load(open('experiments/deeplab.yaml'))\n",
    "class_mapping = cfg['data'].get('class_mapping', {'background':0, 'n':1, 'b':2})\n",
    "num_classes = len(class_mapping)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "set_seed(cfg['logging'].get('seed', 1337))\n",
    "\n",
    "val_ds = WBCDataset(Path(cfg['data']['val_images']), mask_dir=Path(cfg['data']['val_masks']),\n",
    "                    annotation_dir=None, transforms=get_val_transforms(), class_mapping=class_mapping)\n",
    "val_loader = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=2)\n",
    "model = build_model(name=cfg['model']['name'], in_channels=cfg['model']['in_channels'],\n",
    "                    num_classes=cfg['model']['num_classes'],\n",
    "                    pretrained_backbone=cfg['model'].get('pretrained_backbone', False)).to(device)\n",
    "ckpt = torch.load('outputs/deeplab/checkpoints/best.pt', map_location=device)\n",
    "model.load_state_dict(ckpt['state_dict'], strict=False)\n",
    "model.eval()\n",
    "\n",
    "def per_class_dice(pred, target, k):\n",
    "    pred_oh = torch.nn.functional.one_hot(pred, num_classes=k).permute(0,3,1,2)\n",
    "    tgt_oh = torch.nn.functional.one_hot(target, num_classes=k).permute(0,3,1,2)\n",
    "    dims = (0,2,3)\n",
    "    inter = (pred_oh * tgt_oh).sum(dim=dims)\n",
    "    card = (pred_oh + tgt_oh).sum(dim=dims)\n",
    "    return ((2*inter + 1e-6)/(card + 1e-6)).squeeze(0)\n",
    "\n",
    "per_image = []\n",
    "with torch.no_grad():\n",
    "    for i,(imgs,masks) in enumerate(val_loader):\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "        out = model(imgs)\n",
    "        logits = out['out'] if isinstance(out, dict) else out\n",
    "        preds = torch.argmax(torch.softmax(logits, dim=1), dim=1)\n",
    "        d = per_class_dice(preds, masks, num_classes).cpu().numpy()\n",
    "        per_image.append({'idx': i, 'mean_dice': float(d.mean()), 'per_class': d})\n",
    "\n",
    "per_image = sorted(per_image, key=lambda x: x['mean_dice'])\n",
    "mean_per_class = np.mean([p['per_class'] for p in per_image], axis=0)\n",
    "print('Mean Dice per class (bg, nucleus, boundary):', np.round(mean_per_class, 3))\n",
    "print('Worst 5 images by Dice:')\n",
    "for w in per_image[:5]:\n",
    "    print('{} mean {:.3f} per-class {}'.format(w['idx'], w['mean_dice'], np.round(w['per_class'],3)))\n",
    "worst_indices = [w['idx'] for w in per_image[:6]]\n",
    "Path('tmp_worst.npy').write_bytes(np.array(worst_indices, dtype=np.int64).tobytes())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffb5942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show overlays (input / GT / Pred) for a few val samples (including worst cases)\n",
    "import numpy as np, matplotlib.pyplot as plt, cv2\n",
    "from pathlib import Path\n",
    "from src.dataset import WBCDataset\n",
    "from src.transforms import get_val_transforms\n",
    "\n",
    "def colorize(mask):\n",
    "    colors = np.array([[0,0,0],[255,255,255],[255,0,0]], dtype=np.uint8)\n",
    "    mask = np.clip(mask, 0, len(colors)-1)\n",
    "    return colors[mask]\n",
    "\n",
    "val_ds = WBCDataset(Path(cfg['data']['val_images']), mask_dir=Path(cfg['data']['val_masks']),\n",
    "                    annotation_dir=None, transforms=get_val_transforms(), class_mapping=class_mapping)\n",
    "worst_indices = np.frombuffer(Path('tmp_worst.npy').read_bytes(), dtype=np.int64).tolist() if Path('tmp_worst.npy').exists() else []\n",
    "to_show = (worst_indices + list(range(6)))[:6]\n",
    "\n",
    "rows = len(to_show)\n",
    "fig, axes = plt.subplots(rows, 3, figsize=(9, 3*rows))\n",
    "for r, idx in enumerate(to_show):\n",
    "    img, mask = val_ds[idx]\n",
    "    img_np = (img.permute(1,2,0).numpy()*255).astype(np.uint8)\n",
    "    pred_path = Path(f'outputs/deeplab/preds_val/{idx:04d}_0_pred.png')\n",
    "    pred = cv2.cvtColor(cv2.imread(str(pred_path)), cv2.COLOR_BGR2RGB) if pred_path.exists() else np.zeros_like(img_np)\n",
    "    axes[r,0].imshow(img_np); axes[r,0].axis('off'); axes[r,0].set_title(f'Idx {idx} Input')\n",
    "    axes[r,1].imshow(colorize(mask.numpy())); axes[r,1].axis('off'); axes[r,1].set_title('GT')\n",
    "    axes[r,2].imshow(pred); axes[r,2].axis('off'); axes[r,2].set_title('Pred')\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f39b386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview augmentations (image + mask)\n",
    "import matplotlib.pyplot as plt\n",
    "from src.transforms import get_train_transforms\n",
    "train_ds = WBCDataset(Path(cfg['data']['train_images']), mask_dir=Path(cfg['data']['train_masks']),\n",
    "                      annotation_dir=None, transforms=get_train_transforms(cfg), class_mapping=class_mapping)\n",
    "fig, axes = plt.subplots(2, 3, figsize=(9,6))\n",
    "for i in range(6):\n",
    "    img, m = train_ds[i]\n",
    "    img_np = ((img.permute(1,2,0).numpy()*255).clip(0,255)).astype(np.uint8)\n",
    "    axes[i//3, i%3].imshow(img_np)\n",
    "    axes[i//3, i%3].imshow(colorize(m.numpy()), alpha=0.4)\n",
    "    axes[i//3, i%3].axis('off')\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
